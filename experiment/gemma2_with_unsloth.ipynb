{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"4d9ec53e8fd3488c802754d36aadacff":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fe6e0d41b7f1429fb29f6f169a3dd820","IPY_MODEL_08c1c004bbe945b28a7cf23936219000","IPY_MODEL_63a2d114330e400cbd5d5837bd08d5c8"],"layout":"IPY_MODEL_ec16e40b43794c51b08479e8e09096c9"}},"fe6e0d41b7f1429fb29f6f169a3dd820":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2345be2183c44951b5d0f22cbf320a53","placeholder":"​","style":"IPY_MODEL_fc648a19bba34b3b8d2cf00021f97f60","value":"Map (num_proc=4): 100%"}},"08c1c004bbe945b28a7cf23936219000":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_06311a2bbf354307a8f2c435d1fab584","max":10000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_322b03ec2fde4ab9ac7cac606d935a8b","value":10000}},"63a2d114330e400cbd5d5837bd08d5c8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_efab67cdb79841c796824896a86f0112","placeholder":"​","style":"IPY_MODEL_7f962a49a02c4f8ea0016de524911a13","value":" 10000/10000 [00:02&lt;00:00, 3867.40 examples/s]"}},"ec16e40b43794c51b08479e8e09096c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2345be2183c44951b5d0f22cbf320a53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc648a19bba34b3b8d2cf00021f97f60":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"06311a2bbf354307a8f2c435d1fab584":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"322b03ec2fde4ab9ac7cac606d935a8b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"efab67cdb79841c796824896a86f0112":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f962a49a02c4f8ea0016de524911a13":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# kaggle format\n%%capture\n!pip install pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y\n!pip install unsloth\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"id":"K8qiukUrVgvV","execution":{"iopub.status.busy":"2024-09-29T05:51:13.174112Z","iopub.execute_input":"2024-09-29T05:51:13.174579Z","iopub.status.idle":"2024-09-29T05:54:38.069394Z","shell.execute_reply.started":"2024-09-29T05:51:13.174537Z","shell.execute_reply":"2024-09-29T05:54:38.068215Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Colab format\n'''\n%%capture\n!pip install unsloth\n# Also get the latest nightly Unsloth!\n!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n\n# Install Flash Attention 2 for softcapping support\nimport torch\nif torch.cuda.get_device_capability()[0] >= 8:\n    !pip install --no-deps packaging ninja einops \"flash-attn>=2.6.3\"\n'''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/gemma-2-2b-it-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-jdXX25lVo5B","outputId":"4b88fb84-30a2-4bbc-ba2a-3fdc90124236","execution":{"iopub.status.busy":"2024-09-29T05:54:38.071745Z","iopub.execute_input":"2024-09-29T05:54:38.072249Z","iopub.status.idle":"2024-09-29T05:55:16.809002Z","shell.execute_reply.started":"2024-09-29T05:54:38.072203Z","shell.execute_reply":"2024-09-29T05:55:16.808215Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"==((====))==  Unsloth 2024.9.post3: Fast Gemma2 patching. Transformers = 4.45.1.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post1. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb579fbe839e4813852feb8d81fe4724"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/209 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fd1fee515874340a7159897ba1868fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d778e2bacace4ac0be13d13baf618f52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab7a63d3cd414533b7b4df411c70f6ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af1556cbefae429f93d21c18d93f6aab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29dbc21d1d1f409281e953f0c849eac6"}},"metadata":{}}]},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2qxtuta_WJPu","outputId":"a5d30fcd-03ee-49da-ab2e-b1ba005203ee","execution":{"iopub.status.busy":"2024-09-29T05:55:16.810157Z","iopub.execute_input":"2024-09-29T05:55:16.810727Z","iopub.status.idle":"2024-09-29T05:55:21.365576Z","shell.execute_reply.started":"2024-09-29T05:55:16.810691Z","shell.execute_reply":"2024-09-29T05:55:21.364494Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Unsloth 2024.9.post3 patched 26 layers with 26 QKV layers, 26 O layers and 26 MLP layers.\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\nhf_token = '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'\n\nlogin(hf_token)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tyoCKXHrW8_1","outputId":"8f9ffeb0-4a08-47db-82cd-6a67c22bb7b6","execution":{"iopub.status.busy":"2024-09-29T05:55:21.368018Z","iopub.execute_input":"2024-09-29T05:55:21.368358Z","iopub.status.idle":"2024-09-29T05:55:21.463395Z","shell.execute_reply.started":"2024-09-29T05:55:21.368317Z","shell.execute_reply":"2024-09-29T05:55:21.462277Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"torch_dtype = torch.float16\nattn_implementation = \"eager\"","metadata":{"id":"9KLrrVTQYD8w","execution":{"iopub.status.busy":"2024-09-29T05:55:21.464700Z","iopub.execute_input":"2024-09-29T05:55:21.465100Z","iopub.status.idle":"2024-09-29T05:55:21.469590Z","shell.execute_reply.started":"2024-09-29T05:55:21.465056Z","shell.execute_reply":"2024-09-29T05:55:21.468678Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import bitsandbytes as bnb\n\ndef find_all_linear_names(model):\n    cls = bnb.nn.Linear8bitLt\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    return list(lora_module_names)\n\ndef print_mdules(model):\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        print(f'name:{name} - module:{module}')\n\ndef find_all_linear_names_old(model):\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, torch.nn.Linear) or isinstance(module, torch.nn.Embedding) or isinstance(module, torch.nn.Conv2d) or isinstance(module, transformers.pytorch_utils.Conv1D):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    return list(lora_module_names)\n\nmodules = find_all_linear_names(model)","metadata":{"id":"-YgloSUDYJfO","execution":{"iopub.status.busy":"2024-09-29T05:55:21.471103Z","iopub.execute_input":"2024-09-29T05:55:21.471777Z","iopub.status.idle":"2024-09-29T05:55:21.487368Z","shell.execute_reply.started":"2024-09-29T05:55:21.471731Z","shell.execute_reply":"2024-09-29T05:55:21.486517Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\nif False:\n    ds = load_dataset(\"b-mc2/sql-create-context\", split='train')\nelse:\n    ds = load_dataset(\"Clinton/Text-to-sql-v1\", split='train')","metadata":{"id":"3H4OaoC6YLfp","execution":{"iopub.status.busy":"2024-09-29T05:55:21.488848Z","iopub.execute_input":"2024-09-29T05:55:21.489180Z","iopub.status.idle":"2024-09-29T05:55:27.571065Z","shell.execute_reply.started":"2024-09-29T05:55:21.489148Z","shell.execute_reply":"2024-09-29T05:55:27.570294Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/118 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c79e4d2b81840aea4038b0306e72903"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"texttosqlv2.jsonl:   0%|          | 0.00/635M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"350010e3bdc94d67bf2b71e877f8a6ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/262208 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81cf93eae10644a786338fd0806f7fa1"}},"metadata":{}}]},{"cell_type":"code","source":"count = 10000\niteration = 0\nstart_index = count * iteration\nend_index = min(start_index + count, len(ds))\nprint(f'{start_index} ~ {end_index}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SdRfe-YqYMva","outputId":"0bf8b929-ec6a-4f24-e7ee-94d12566aa21","execution":{"iopub.status.busy":"2024-09-29T05:55:27.572305Z","iopub.execute_input":"2024-09-29T05:55:27.572616Z","iopub.status.idle":"2024-09-29T05:55:27.578410Z","shell.execute_reply.started":"2024-09-29T05:55:27.572582Z","shell.execute_reply":"2024-09-29T05:55:27.577279Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"0 ~ 10000\n","output_type":"stream"}]},{"cell_type":"code","source":"ds = ds.select(range(start_index, end_index))\nds","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7gnCcxzFYNk6","outputId":"1945ed17-66b6-4847-fc16-e564d7da8ff9","execution":{"iopub.status.busy":"2024-09-29T05:55:27.579583Z","iopub.execute_input":"2024-09-29T05:55:27.579862Z","iopub.status.idle":"2024-09-29T05:55:27.612846Z","shell.execute_reply.started":"2024-09-29T05:55:27.579831Z","shell.execute_reply":"2024-09-29T05:55:27.611916Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['instruction', 'input', 'response', 'source', 'text'],\n    num_rows: 10000\n})"},"metadata":{}}]},{"cell_type":"code","source":"def format_chat_template(row):\n    row_json = [\n        {\"role\": \"user\", \"content\": f\"\"\"\nUse the below SQL tables schemas paired with instruction that describes a task. make SQL query that appropriately completes the request for the provided tables. And make SQL query according the steps.\n{row[\"input\"]}\nstep 1. check columns that user wants.\nstep 2. check condition that user wants.\nstep 3. make SQL query to get every information that user wants.\n\n{row[\"instruction\"]}\n\"\"\"},\n        {\"role\": \"model\", \"content\": row[\"response\"]}\n    ]\n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\nds = ds.map(\n    format_chat_template,\n    num_proc=4,\n)\n\nds","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":120,"referenced_widgets":["4d9ec53e8fd3488c802754d36aadacff","fe6e0d41b7f1429fb29f6f169a3dd820","08c1c004bbe945b28a7cf23936219000","63a2d114330e400cbd5d5837bd08d5c8","ec16e40b43794c51b08479e8e09096c9","2345be2183c44951b5d0f22cbf320a53","fc648a19bba34b3b8d2cf00021f97f60","06311a2bbf354307a8f2c435d1fab584","322b03ec2fde4ab9ac7cac606d935a8b","efab67cdb79841c796824896a86f0112","7f962a49a02c4f8ea0016de524911a13"]},"id":"GQs3H3FSYOZy","outputId":"06bba014-343c-48c2-96b8-49b558fabf55","execution":{"iopub.status.busy":"2024-09-29T05:55:27.616050Z","iopub.execute_input":"2024-09-29T05:55:27.616313Z","iopub.status.idle":"2024-09-29T05:55:29.279279Z","shell.execute_reply.started":"2024-09-29T05:55:27.616284Z","shell.execute_reply":"2024-09-29T05:55:29.278215Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08bbc6c0393e46f5ac0f3fcb7f8832b7"}},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['instruction', 'input', 'response', 'source', 'text'],\n    num_rows: 10000\n})"},"metadata":{}}]},{"cell_type":"code","source":"ds['text'][0]","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":160},"id":"MP7XM3_TYPQr","outputId":"f16d228f-0e68-44ab-a6aa-83b93375b721","execution":{"iopub.status.busy":"2024-09-29T05:55:29.280709Z","iopub.execute_input":"2024-09-29T05:55:29.281048Z","iopub.status.idle":"2024-09-29T05:55:29.319824Z","shell.execute_reply.started":"2024-09-29T05:55:29.281013Z","shell.execute_reply":"2024-09-29T05:55:29.318980Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'<bos><start_of_turn>user\\nUse the below SQL tables schemas paired with instruction that describes a task. make SQL query that appropriately completes the request for the provided tables. And make SQL query according the steps.\\nCREATE TABLE table_name_77 (\\n    home_team VARCHAR,\\n    away_team VARCHAR\\n)\\nstep 1. check columns that user wants.\\nstep 2. check condition that user wants.\\nstep 3. make SQL query to get every information that user wants.\\n\\nName the home team for carlton away team<end_of_turn>\\n<start_of_turn>model\\nSELECT home_team FROM table_name_77 WHERE away_team = \"carlton\"<end_of_turn>\\n'"},"metadata":{}}]},{"cell_type":"code","source":"ds = ds.train_test_split(test_size=0.01)\nds","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zoyftYUQYQZv","outputId":"f9e6c39a-a31b-47f2-bc35-21c08bc4f6e6","execution":{"iopub.status.busy":"2024-09-29T05:55:29.321539Z","iopub.execute_input":"2024-09-29T05:55:29.321836Z","iopub.status.idle":"2024-09-29T05:55:29.392254Z","shell.execute_reply.started":"2024-09-29T05:55:29.321804Z","shell.execute_reply":"2024-09-29T05:55:29.391381Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['instruction', 'input', 'response', 'source', 'text'],\n        num_rows: 9900\n    })\n    test: Dataset({\n        features: ['instruction', 'input', 'response', 'source', 'text'],\n        num_rows: 100\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"import torch.cuda\nimport torch\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\nfrom tensorflow.keras.optimizers import Adam","metadata":{"id":"jYoNyPvTYQtq","execution":{"iopub.status.busy":"2024-09-29T05:55:29.393249Z","iopub.execute_input":"2024-09-29T05:55:29.393514Z","iopub.status.idle":"2024-09-29T05:55:29.457875Z","shell.execute_reply.started":"2024-09-29T05:55:29.393485Z","shell.execute_reply":"2024-09-29T05:55:29.457179Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntraining_arguments = TrainingArguments(\n    output_dir='output',\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim = \"adamw_8bit\",\n    num_train_epochs=1,\n    eval_strategy=\"steps\",\n    eval_steps=0.2,\n    warmup_steps = 10,\n#    max_steps = 1000,\n    learning_rate = 2e-4,\n    fp16 = not is_bfloat16_supported(),\n    bf16 = is_bfloat16_supported(),\n    logging_steps = 10,\n    weight_decay = 0.01,\n    lr_scheduler_type = \"linear\",\n)\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset=ds[\"train\"],\n    eval_dataset=ds[\"test\"],\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = training_arguments,\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cbeSkXzbYSCA","outputId":"ad4c43be-f8e6-4c71-b9ad-d21ed5140f4e","execution":{"iopub.status.busy":"2024-09-29T05:58:40.755287Z","iopub.execute_input":"2024-09-29T05:58:40.755700Z","iopub.status.idle":"2024-09-29T05:58:53.133146Z","shell.execute_reply.started":"2024-09-29T05:58:40.755659Z","shell.execute_reply":"2024-09-29T05:58:53.132162Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/9900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bc02f85bd714d8ba56f238dd66b416a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76277eb9922a4676b4281829a5610833"}},"metadata":{}}]},{"cell_type":"code","source":"model.config.use_cache = False\ntrainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393},"id":"3MX79MbyYWqs","outputId":"5365859f-0053-4a8b-b519-683ccb91edc6","execution":{"iopub.status.busy":"2024-09-29T05:58:53.135068Z","iopub.execute_input":"2024-09-29T05:58:53.135400Z","iopub.status.idle":"2024-09-29T07:49:48.161769Z","shell.execute_reply.started":"2024-09-29T05:58:53.135363Z","shell.execute_reply":"2024-09-29T07:49:48.160882Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 9,900 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 2\n\\        /    Total batch size = 2 | Total steps = 4,950\n \"-____-\"     Number of trainable parameters = 20,766,720\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4950' max='4950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4950/4950 1:50:50, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>990</td>\n      <td>0.359800</td>\n      <td>0.322311</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.284900</td>\n      <td>0.293610</td>\n    </tr>\n    <tr>\n      <td>2970</td>\n      <td>0.294000</td>\n      <td>0.274669</td>\n    </tr>\n    <tr>\n      <td>3960</td>\n      <td>0.329400</td>\n      <td>0.262802</td>\n    </tr>\n    <tr>\n      <td>4950</td>\n      <td>0.286700</td>\n      <td>0.257756</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=4950, training_loss=0.34689224175732547, metrics={'train_runtime': 6651.3311, 'train_samples_per_second': 1.488, 'train_steps_per_second': 0.744, 'total_flos': 6.1175363889024e+16, 'train_loss': 0.34689224175732547, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"FastLanguageModel.for_inference(model)\ntable_schema = 'CREATE TABLE person ( name VARCHAR, age INTEGER, address VARCHAR )'\nuser_qery = 'people whoes ages are older than 27 and name starts with k'\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": f\"\"\"Use the below SQL tables schemas paired with instruction that describes a task. make SQL query that appropriately completes the request for the provided tables. And make SQL query according the steps.\n{table_schema}\nstep 1. check columns that user wants.\nstep 2. check condition that user wants.\nstep 3. make SQL query to get every information that user wants.\n\n{user_qery}\n\"\"\"\n    }\n]\n\nformated_messages = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\")\ninput_ids = tokenizer(formated_messages, return_tensors=\"pt\")","metadata":{"id":"qNVlzS2da8qE","execution":{"iopub.status.busy":"2024-09-29T07:52:09.177923Z","iopub.execute_input":"2024-09-29T07:52:09.178324Z","iopub.status.idle":"2024-09-29T07:52:09.186719Z","shell.execute_reply.started":"2024-09-29T07:52:09.178288Z","shell.execute_reply":"2024-09-29T07:52:09.185592Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"formated_messages","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"sZU_ezLOby1A","outputId":"d44ab5af-8ea8-4573-bebe-80ba576e5160","execution":{"iopub.status.busy":"2024-09-29T07:52:11.880819Z","iopub.execute_input":"2024-09-29T07:52:11.881230Z","iopub.status.idle":"2024-09-29T07:52:11.887828Z","shell.execute_reply.started":"2024-09-29T07:52:11.881192Z","shell.execute_reply":"2024-09-29T07:52:11.886758Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"'<bos><start_of_turn>user\\nUse the below SQL tables schemas paired with instruction that describes a task. make SQL query that appropriately completes the request for the provided tables. And make SQL query according the steps.\\nCREATE TABLE person ( name VARCHAR, age INTEGER, address VARCHAR )\\nstep 1. check columns that user wants.\\nstep 2. check condition that user wants.\\nstep 3. make SQL query to get every information that user wants.\\n\\npeople whoes ages are older than 27 and name starts with k<end_of_turn>\\n<start_of_turn>model\\n'"},"metadata":{}}]},{"cell_type":"code","source":"response = model.generate(**input_ids, max_new_tokens=32, repetition_penalty=1.1)\nresponse = str(tokenizer.batch_decode(response))\n#print(str(response))\nprint(f'assistant: {response[response.find(\"<start_of_turn>model\")+22:-1].strip()}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ydw1kqtaWVb","outputId":"d6279ae1-48be-483a-fb53-86279e1175c4","execution":{"iopub.status.busy":"2024-09-29T07:59:03.131143Z","iopub.execute_input":"2024-09-29T07:59:03.131499Z","iopub.status.idle":"2024-09-29T07:59:05.390334Z","shell.execute_reply.started":"2024-09-29T07:59:03.131463Z","shell.execute_reply":"2024-09-29T07:59:05.389319Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"assistant: SELECT T1.name FROM person AS T1 JOIN person AS T2 ON T1.age > 27 WHERE T1.name LIKE '%k\"\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save_pretrained(\"gemma-2-2b-it-nl2sql\") # Local saving\ntokenizer.save_pretrained(\"gemma-2-2b-it-nl2sql\")\nmodel.push_to_hub(\"gemma-2-2b-it-nl2sql\") # Online saving\ntokenizer.push_to_hub(\"gemma-2-2b-it-nl2sql\") # Online saving","metadata":{"id":"eFa4XngDbr4M","execution":{"iopub.status.busy":"2024-09-29T07:59:09.512091Z","iopub.execute_input":"2024-09-29T07:59:09.512713Z","iopub.status.idle":"2024-09-29T07:59:14.786751Z","shell.execute_reply.started":"2024-09-29T07:59:09.512674Z","shell.execute_reply":"2024-09-29T07:59:14.785821Z"},"trusted":true},"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"729ae67922624f8ca7034775412bbb81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"282a4fc157c04f089e7c9c9bde0e4b5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/83.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c945ca3968244384b607b7a71bcf8cf6"}},"metadata":{}},{"name":"stdout","text":"Saved model to https://huggingface.co/gemma-2-2b-it-nl2sql\n","output_type":"stream"},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save_pretrained_merged(\"gemma-2-2b-it-nl2sql\", tokenizer, save_method = \"merged_16bit\",)\nmodel.push_to_hub_merged(\"gemma-2-2b-it-nl2sql\", tokenizer, save_method = \"merged_16bit\")","metadata":{"id":"x2zRZj51c-4j","execution":{"iopub.status.busy":"2024-09-29T07:59:14.788231Z","iopub.execute_input":"2024-09-29T07:59:14.788543Z","iopub.status.idle":"2024-09-29T08:00:49.771153Z","shell.execute_reply.started":"2024-09-29T07:59:14.788509Z","shell.execute_reply":"2024-09-29T08:00:49.770092Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"Unsloth: You have 2 CPUs. Using `safe_serialization` is 10x slower.\nWe shall switch to Pytorch saving, which will take 3 minutes and not 30 minutes.\nTo force `safe_serialization`, set it to `None` instead.\nUnsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\nmodel which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\nUnsloth: Will remove a cached repo with size 2.2G\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 19.31 out of 31.36 RAM for saving.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 26/26 [00:00<00:00, 35.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Saving tokenizer... Done.\nUnsloth: Saving model... This might take 5 minutes for Llama-7b...\nUnsloth: Saving gemma-2-2b-it-nl2sql/pytorch_model-00001-of-00002.bin...\nUnsloth: Saving gemma-2-2b-it-nl2sql/pytorch_model-00002-of-00002.bin...\nDone.\nUnsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 19.29 out of 31.36 RAM for saving.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 26/26 [00:00<00:00, 38.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Saving to organization with address 9-coding/gemma-2-2b-it-nl2sql\nUnsloth: Saving tokenizer... Done.\nUnsloth: Saving model... This might take 5 minutes for Llama-7b...\nUnsloth: Saving to organization with address 9-coding/gemma-2-2b-it-nl2sql\nUnsloth: Saving 9-coding/gemma-2-2b-it-nl2sql/pytorch_model-00001-of-00002.bin...\nUnsloth: Saving 9-coding/gemma-2-2b-it-nl2sql/pytorch_model-00002-of-00002.bin...\nUnsloth: Uploading all files... Please wait...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bb8ed6543a04e82852534ddbd0ba1e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f51d4833454470e8ede9e812ecb71e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"502f801a9fa345c6988b51fe97d049ab"}},"metadata":{}},{"name":"stdout","text":"Done.\nSaved merged model to https://huggingface.co/None/gemma-2-2b-it-nl2sql\n","output_type":"stream"}]}]}