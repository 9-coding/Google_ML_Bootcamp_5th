{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/9-coding/Google_ML_Bootcamp_5th/blob/main/NL2SQL_unsloth_gemma2_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-28T01:49:53.742851Z",
          "iopub.status.busy": "2024-09-28T01:49:53.741834Z",
          "iopub.status.idle": "2024-09-28T01:50:24.645525Z",
          "shell.execute_reply": "2024-09-28T01:50:24.643923Z",
          "shell.execute_reply.started": "2024-09-28T01:49:53.742781Z"
        },
        "trusted": true,
        "id": "CQaLllgGwA7o"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-28T01:50:24.648692Z",
          "iopub.status.busy": "2024-09-28T01:50:24.648258Z",
          "iopub.status.idle": "2024-09-28T01:50:29.560084Z",
          "shell.execute_reply": "2024-09-28T01:50:29.558710Z",
          "shell.execute_reply.started": "2024-09-28T01:50:24.648647Z"
        },
        "trusted": true,
        "id": "KtrnCHx-wA7p"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datetime import datetime\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-28T01:50:29.562114Z",
          "iopub.status.busy": "2024-09-28T01:50:29.561570Z",
          "iopub.status.idle": "2024-09-28T01:50:29.571407Z",
          "shell.execute_reply": "2024-09-28T01:50:29.569792Z",
          "shell.execute_reply.started": "2024-09-28T01:50:29.562070Z"
        },
        "trusted": true,
        "id": "PwrrO5PVwA7p"
      },
      "outputs": [],
      "source": [
        "use_gpu = torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-28T01:53:08.064709Z",
          "iopub.status.busy": "2024-09-28T01:53:08.061813Z",
          "iopub.status.idle": "2024-09-28T01:53:08.075891Z",
          "shell.execute_reply": "2024-09-28T01:53:08.074540Z",
          "shell.execute_reply.started": "2024-09-28T01:53:08.064612Z"
        },
        "trusted": true,
        "id": "zx5k5K7zwA7q"
      },
      "outputs": [],
      "source": [
        "def extract_response(result):\n",
        "    start_token = \"<start_of_turn>model\"\n",
        "    end_token = \"<end_of_turn>\"\n",
        "\n",
        "    # 시작점과 끝점 인덱스 찾기\n",
        "    start_index = result.find(start_token)\n",
        "    if start_index == -1:\n",
        "        return None  # 시작 토큰이 없으면 None 반환\n",
        "    start_index += len(start_token)\n",
        "\n",
        "    end_index = result.find(end_token, start_index)\n",
        "    if end_index == -1:\n",
        "        return result[start_index:].strip()\n",
        "\n",
        "    # 시작점과 끝점 사이의 텍스트 잘라내기\n",
        "    return result[start_index:end_index].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-28T01:53:33.389787Z",
          "iopub.status.busy": "2024-09-28T01:53:33.386500Z",
          "iopub.status.idle": "2024-09-28T01:53:33.406720Z",
          "shell.execute_reply": "2024-09-28T01:53:33.404489Z",
          "shell.execute_reply.started": "2024-09-28T01:53:33.389679Z"
        },
        "trusted": true,
        "id": "-YVczzenwA7q"
      },
      "outputs": [],
      "source": [
        "def send_message(model, tokenizer, messages):\n",
        "    global use_gpu\n",
        "\n",
        "    formated_messages = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\")\n",
        "    input_ids = tokenizer(formated_messages, return_tensors=\"pt\")\n",
        "    if use_gpu:\n",
        "        input_ids = input_ids.to('cuda')\n",
        "\n",
        "    start_time = datetime.now()\n",
        "    print('generate start')\n",
        "\n",
        "    outputs = model.generate(**input_ids, max_new_tokens=32, repetition_penalty=1.1)\n",
        "\n",
        "    end_time = datetime.now()\n",
        "    print('generate done')\n",
        "    print('elapsed time:', (end_time - start_time).total_seconds())\n",
        "\n",
        "    print(tokenizer.decode(outputs[0]) + '\\n\\n\\n')\n",
        "    return extract_response(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-28T01:50:29.837851Z",
          "iopub.status.busy": "2024-09-28T01:50:29.837494Z",
          "iopub.status.idle": "2024-09-28T01:50:29.987343Z",
          "shell.execute_reply": "2024-09-28T01:50:29.985262Z",
          "shell.execute_reply.started": "2024-09-28T01:50:29.837812Z"
        },
        "trusted": true,
        "id": "Jy3aFMKbwA7q",
        "outputId": "e3984d0d-5997-42e9-9a21-b111fc7a29d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "hf_token = 'TOKEN'\n",
        "\n",
        "login(hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-28T01:50:29.990699Z",
          "iopub.status.busy": "2024-09-28T01:50:29.990077Z",
          "iopub.status.idle": "2024-09-28T01:52:15.332775Z",
          "shell.execute_reply": "2024-09-28T01:52:15.331507Z",
          "shell.execute_reply.started": "2024-09-28T01:50:29.990641Z"
        },
        "trusted": true,
        "id": "xwS6kJEiwA7r",
        "outputId": "ef61950c-8772-493c-95a7-d0b7b89ba62a",
        "colab": {
          "referenced_widgets": [
            "e58b8ec21b6941e5909e2fdaa48f96e4",
            "b160bc48caad4ecabdd6131daef116f8",
            "0275a921defd4e81884a0f9a6fd43785",
            "a9fbca1020894662a5dbc1caa165af3d",
            "dafaba52777b4b72977c4cdd8b8ea970",
            "9105634260874b769d5a8f6d7c5a1f6f",
            "36653b9ed7494c3a8c6f3f5ba565fac7",
            "4b0f586404304771b511ed076a3f1dee",
            "cf9fd7c1b89049f5ae82475be73aa61f",
            "15634733b7364598b6e2af8b43e92647",
            "55f1eaf9aff64b9cbae23a63386eb2ed"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e58b8ec21b6941e5909e2fdaa48f96e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/880 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b160bc48caad4ecabdd6131daef116f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0275a921defd4e81884a0f9a6fd43785",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9fbca1020894662a5dbc1caa165af3d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dafaba52777b4b72977c4cdd8b8ea970",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9105634260874b769d5a8f6d7c5a1f6f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36653b9ed7494c3a8c6f3f5ba565fac7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b0f586404304771b511ed076a3f1dee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf9fd7c1b89049f5ae82475be73aa61f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15634733b7364598b6e2af8b43e92647",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55f1eaf9aff64b9cbae23a63386eb2ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"9-coding/gemma-2-2b-it-nl2sql\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"9-coding/gemma-2-2b-it-nl2sql\", trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-28T01:52:15.335194Z",
          "iopub.status.busy": "2024-09-28T01:52:15.334429Z",
          "iopub.status.idle": "2024-09-28T01:52:15.341523Z",
          "shell.execute_reply": "2024-09-28T01:52:15.340021Z",
          "shell.execute_reply.started": "2024-09-28T01:52:15.335136Z"
        },
        "trusted": true,
        "id": "9vi7Ids6wA7r"
      },
      "outputs": [],
      "source": [
        "if use_gpu:\n",
        "    model.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-28T01:52:15.343587Z",
          "iopub.status.busy": "2024-09-28T01:52:15.343082Z",
          "iopub.status.idle": "2024-09-28T01:52:15.355202Z",
          "shell.execute_reply": "2024-09-28T01:52:15.353833Z",
          "shell.execute_reply.started": "2024-09-28T01:52:15.343531Z"
        },
        "trusted": true,
        "id": "MCa4mYOiwA7r"
      },
      "outputs": [],
      "source": [
        "table_schema = 'CREATE TABLE person ( name VARCHAR, age INTEGER, address VARCHAR )'\n",
        "user_qery = 'people whoes ages are older than 27 and name starts with k'\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"Use the below SQL tables schemas paired with instruction that describes a task. make SQL query that appropriately completes the request for the provided tables. And make SQL query according the steps.\n",
        "{table_schema}\n",
        "step 1. check columns that user wants.\n",
        "step 2. check condition that user wants.\n",
        "step 3. make SQL query to get every information that user wants.\n",
        "\n",
        "{user_qery}\n",
        "\"\"\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-28T01:53:50.370661Z",
          "iopub.status.busy": "2024-09-28T01:53:50.370140Z",
          "iopub.status.idle": "2024-09-28T01:54:03.340245Z",
          "shell.execute_reply": "2024-09-28T01:54:03.338428Z",
          "shell.execute_reply.started": "2024-09-28T01:53:50.370613Z"
        },
        "trusted": true,
        "id": "Q22dH59dwA7r",
        "outputId": "530fadb3-3ee9-4651-ebbd-a749c0486684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generate start\n",
            "generate done\n",
            "elapsed time: 12.9594\n",
            "<bos><bos><start_of_turn>user\n",
            "Use the below SQL tables schemas paired with instruction that describes a task. make SQL query that appropriately completes the request for the provided tables. And make SQL query according the steps.\n",
            "CREATE TABLE person ( name VARCHAR, age INTEGER, address VARCHAR )\n",
            "step 1. check columns that user wants.\n",
            "step 2. check condition that user wants.\n",
            "step 3. make SQL query to get every information that user wants.\n",
            "\n",
            "people whoes ages are older than 27 and name starts with k<end_of_turn>\n",
            "<start_of_turn>model\n",
            "SELECT name FROM person WHERE age > 27 AND name LIKE \"k%\"<end_of_turn>\n",
            "\n",
            "\n",
            "\n",
            "===============================================\n",
            "assistant: SELECT name FROM person WHERE age > 27 AND name LIKE \"k%\"\n"
          ]
        }
      ],
      "source": [
        "response = send_message(model, tokenizer, messages)\n",
        "print('===============================================')\n",
        "print(f'assistant: {response}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PULjpo0fwA7r"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30761,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}